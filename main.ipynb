{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_model import train_model\n",
    "from model import ShakespearModel\n",
    "\n",
    "N_EPOCHS = 2\n",
    "N_TOKENS = 250  # N\n",
    "N_LAYERS = 6  # L\n",
    "N_HEADS = 4  # h\n",
    "N_WORKERS = 2\n",
    "BATCH_SIZE = 5  # B\n",
    "D_MODEL = 200  # d\n",
    "D_K = 32\n",
    "D_V = D_K\n",
    "D_FF = 512\n",
    "RAW_DATA_PATH = './datasets/shakespear_corpus.txt'\n",
    "\n",
    "#trained_mod = train_model(N_EPOCHS, N_TOKENS, N_LAYERS, N_HEADS, BATCH_SIZE, D_MODEL, D_K, D_V, D_FF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(134.6081)\n"
     ]
    }
   ],
   "source": [
    "from train_model import load_data\n",
    "from parsing.CharDataSet import CharDataSet\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "raw_data = load_data(RAW_DATA_PATH)\n",
    "\n",
    "tokenized_data = CharDataSet(N_TOKENS, raw_data)\n",
    "data_loader = DataLoader(\n",
    "    tokenized_data,\n",
    "    shuffle=False,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=N_WORKERS,\n",
    ")\n",
    "\n",
    "trained_mod = ShakespearModel(N_LAYERS, N_HEADS, D_MODEL, D_FF, D_K, D_V, BATCH_SIZE, N_TOKENS, tokenized_data.get_vocab_size())\n",
    "print(trained_mod.param_norm())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = \"O God, O God!\"\n",
    "idx = tokenized_data.encode(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(134.6081)\n",
      "idx tensor([[27.,  1., 19., 53., 42.,  6.,  1., 27.,  1., 19., 53., 42.,  2.]])\n",
      "tensor(134.6081)\n",
      "tensor([27.,  1., 19., 53., 42.,  6.,  1., 27.,  1., 19., 53., 42.,  2., 15.,\n",
      "        15., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 30., 30., 30.,\n",
      "        30., 30., 30., 11., 11., 30., 30., 11., 11., 11., 11., 11., 11., 30.,\n",
      "        30., 30., 30., 30., 30., 29., 29., 29., 29., 29., 29., 29., 29., 29.,\n",
      "        29., 30., 30., 30., 30., 29., 11.])\n"
     ]
    }
   ],
   "source": [
    "print(trained_mod.param_norm())\n",
    "new_tokens = trained_mod.generate(idx, n_new_tokens=50)\n",
    "print(trained_mod.param_norm())\n",
    "print(new_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O God, O God!CC;;;;;;;;;;RRRRRR;;RR;;;;;;RRRRRRQQQQQQQQQQRRRRQ;\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_data.decode(new_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WTE.weight torch.Size([65, 200])\n",
      "transformer.blocks.0.CausalSelfAttn.to_Q.weight torch.Size([128, 200])\n",
      "transformer.blocks.0.CausalSelfAttn.to_Q.bias torch.Size([128])\n",
      "transformer.blocks.0.CausalSelfAttn.to_K.weight torch.Size([128, 200])\n",
      "transformer.blocks.0.CausalSelfAttn.to_K.bias torch.Size([128])\n",
      "transformer.blocks.0.CausalSelfAttn.to_V.weight torch.Size([128, 200])\n",
      "transformer.blocks.0.CausalSelfAttn.to_V.bias torch.Size([128])\n",
      "transformer.blocks.0.CausalSelfAttn.W_O.weight torch.Size([200, 128])\n",
      "transformer.blocks.0.CausalSelfAttn.W_O.bias torch.Size([200])\n",
      "transformer.blocks.0.LayerNorm_1.weight torch.Size([200])\n",
      "transformer.blocks.0.LayerNorm_1.bias torch.Size([200])\n",
      "transformer.blocks.0.FFN.L1.weight torch.Size([512, 200])\n",
      "transformer.blocks.0.FFN.L1.bias torch.Size([512])\n",
      "transformer.blocks.0.FFN.L2.weight torch.Size([200, 512])\n",
      "transformer.blocks.0.FFN.L2.bias torch.Size([200])\n",
      "transformer.blocks.0.LayerNorm_2.weight torch.Size([200])\n",
      "transformer.blocks.0.LayerNorm_2.bias torch.Size([200])\n",
      "transformer.blocks.1.CausalSelfAttn.to_Q.weight torch.Size([128, 200])\n",
      "transformer.blocks.1.CausalSelfAttn.to_Q.bias torch.Size([128])\n",
      "transformer.blocks.1.CausalSelfAttn.to_K.weight torch.Size([128, 200])\n",
      "transformer.blocks.1.CausalSelfAttn.to_K.bias torch.Size([128])\n",
      "transformer.blocks.1.CausalSelfAttn.to_V.weight torch.Size([128, 200])\n",
      "transformer.blocks.1.CausalSelfAttn.to_V.bias torch.Size([128])\n",
      "transformer.blocks.1.CausalSelfAttn.W_O.weight torch.Size([200, 128])\n",
      "transformer.blocks.1.CausalSelfAttn.W_O.bias torch.Size([200])\n",
      "transformer.blocks.1.LayerNorm_1.weight torch.Size([200])\n",
      "transformer.blocks.1.LayerNorm_1.bias torch.Size([200])\n",
      "transformer.blocks.1.FFN.L1.weight torch.Size([512, 200])\n",
      "transformer.blocks.1.FFN.L1.bias torch.Size([512])\n",
      "transformer.blocks.1.FFN.L2.weight torch.Size([200, 512])\n",
      "transformer.blocks.1.FFN.L2.bias torch.Size([200])\n",
      "transformer.blocks.1.LayerNorm_2.weight torch.Size([200])\n",
      "transformer.blocks.1.LayerNorm_2.bias torch.Size([200])\n",
      "transformer.blocks.2.CausalSelfAttn.to_Q.weight torch.Size([128, 200])\n",
      "transformer.blocks.2.CausalSelfAttn.to_Q.bias torch.Size([128])\n",
      "transformer.blocks.2.CausalSelfAttn.to_K.weight torch.Size([128, 200])\n",
      "transformer.blocks.2.CausalSelfAttn.to_K.bias torch.Size([128])\n",
      "transformer.blocks.2.CausalSelfAttn.to_V.weight torch.Size([128, 200])\n",
      "transformer.blocks.2.CausalSelfAttn.to_V.bias torch.Size([128])\n",
      "transformer.blocks.2.CausalSelfAttn.W_O.weight torch.Size([200, 128])\n",
      "transformer.blocks.2.CausalSelfAttn.W_O.bias torch.Size([200])\n",
      "transformer.blocks.2.LayerNorm_1.weight torch.Size([200])\n",
      "transformer.blocks.2.LayerNorm_1.bias torch.Size([200])\n",
      "transformer.blocks.2.FFN.L1.weight torch.Size([512, 200])\n",
      "transformer.blocks.2.FFN.L1.bias torch.Size([512])\n",
      "transformer.blocks.2.FFN.L2.weight torch.Size([200, 512])\n",
      "transformer.blocks.2.FFN.L2.bias torch.Size([200])\n",
      "transformer.blocks.2.LayerNorm_2.weight torch.Size([200])\n",
      "transformer.blocks.2.LayerNorm_2.bias torch.Size([200])\n",
      "transformer.blocks.3.CausalSelfAttn.to_Q.weight torch.Size([128, 200])\n",
      "transformer.blocks.3.CausalSelfAttn.to_Q.bias torch.Size([128])\n",
      "transformer.blocks.3.CausalSelfAttn.to_K.weight torch.Size([128, 200])\n",
      "transformer.blocks.3.CausalSelfAttn.to_K.bias torch.Size([128])\n",
      "transformer.blocks.3.CausalSelfAttn.to_V.weight torch.Size([128, 200])\n",
      "transformer.blocks.3.CausalSelfAttn.to_V.bias torch.Size([128])\n",
      "transformer.blocks.3.CausalSelfAttn.W_O.weight torch.Size([200, 128])\n",
      "transformer.blocks.3.CausalSelfAttn.W_O.bias torch.Size([200])\n",
      "transformer.blocks.3.LayerNorm_1.weight torch.Size([200])\n",
      "transformer.blocks.3.LayerNorm_1.bias torch.Size([200])\n",
      "transformer.blocks.3.FFN.L1.weight torch.Size([512, 200])\n",
      "transformer.blocks.3.FFN.L1.bias torch.Size([512])\n",
      "transformer.blocks.3.FFN.L2.weight torch.Size([200, 512])\n",
      "transformer.blocks.3.FFN.L2.bias torch.Size([200])\n",
      "transformer.blocks.3.LayerNorm_2.weight torch.Size([200])\n",
      "transformer.blocks.3.LayerNorm_2.bias torch.Size([200])\n",
      "transformer.blocks.4.CausalSelfAttn.to_Q.weight torch.Size([128, 200])\n",
      "transformer.blocks.4.CausalSelfAttn.to_Q.bias torch.Size([128])\n",
      "transformer.blocks.4.CausalSelfAttn.to_K.weight torch.Size([128, 200])\n",
      "transformer.blocks.4.CausalSelfAttn.to_K.bias torch.Size([128])\n",
      "transformer.blocks.4.CausalSelfAttn.to_V.weight torch.Size([128, 200])\n",
      "transformer.blocks.4.CausalSelfAttn.to_V.bias torch.Size([128])\n",
      "transformer.blocks.4.CausalSelfAttn.W_O.weight torch.Size([200, 128])\n",
      "transformer.blocks.4.CausalSelfAttn.W_O.bias torch.Size([200])\n",
      "transformer.blocks.4.LayerNorm_1.weight torch.Size([200])\n",
      "transformer.blocks.4.LayerNorm_1.bias torch.Size([200])\n",
      "transformer.blocks.4.FFN.L1.weight torch.Size([512, 200])\n",
      "transformer.blocks.4.FFN.L1.bias torch.Size([512])\n",
      "transformer.blocks.4.FFN.L2.weight torch.Size([200, 512])\n",
      "transformer.blocks.4.FFN.L2.bias torch.Size([200])\n",
      "transformer.blocks.4.LayerNorm_2.weight torch.Size([200])\n",
      "transformer.blocks.4.LayerNorm_2.bias torch.Size([200])\n",
      "transformer.blocks.5.CausalSelfAttn.to_Q.weight torch.Size([128, 200])\n",
      "transformer.blocks.5.CausalSelfAttn.to_Q.bias torch.Size([128])\n",
      "transformer.blocks.5.CausalSelfAttn.to_K.weight torch.Size([128, 200])\n",
      "transformer.blocks.5.CausalSelfAttn.to_K.bias torch.Size([128])\n",
      "transformer.blocks.5.CausalSelfAttn.to_V.weight torch.Size([128, 200])\n",
      "transformer.blocks.5.CausalSelfAttn.to_V.bias torch.Size([128])\n",
      "transformer.blocks.5.CausalSelfAttn.W_O.weight torch.Size([200, 128])\n",
      "transformer.blocks.5.CausalSelfAttn.W_O.bias torch.Size([200])\n",
      "transformer.blocks.5.LayerNorm_1.weight torch.Size([200])\n",
      "transformer.blocks.5.LayerNorm_1.bias torch.Size([200])\n",
      "transformer.blocks.5.FFN.L1.weight torch.Size([512, 200])\n",
      "transformer.blocks.5.FFN.L1.bias torch.Size([512])\n",
      "transformer.blocks.5.FFN.L2.weight torch.Size([200, 512])\n",
      "transformer.blocks.5.FFN.L2.bias torch.Size([200])\n",
      "transformer.blocks.5.LayerNorm_2.weight torch.Size([200])\n",
      "transformer.blocks.5.LayerNorm_2.bias torch.Size([200])\n",
      "transformer.Final_LayerNorm.weight torch.Size([200])\n",
      "transformer.Final_LayerNorm.bias torch.Size([200])\n",
      "transformer.LM_Head.to_V.weight torch.Size([65, 200])\n",
      "transformer.LM_Head.to_V.bias torch.Size([65])\n"
     ]
    }
   ],
   "source": [
    "for name, param in trained_mod.named_parameters():\n",
    "    print(name, param.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in trained_mod.transformer.blocks[0].CausalSelfAttn.named_parameters():\n",
    "    print(name, param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
