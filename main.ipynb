{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import getLoaderDataset\n",
    "\n",
    "N = 256\n",
    "B = 5\n",
    "\n",
    "loader, dataset = getLoaderDataset(N, B, \"./datasets/shakespear_corpus.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1115137\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import GPT, train_model\n",
    "\n",
    "L = 6\n",
    "d = 384\n",
    "d_ff = 4 * d\n",
    "h = 6\n",
    "V = dataset.get_vocab_size()\n",
    "\n",
    "model = GPT(B, L, d, d_ff, N, h, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0, Loss : 4.304887771606445\n",
      "batch 1, Loss : 3.9289886951446533\n",
      "batch 2, Loss : 3.684896945953369\n",
      "batch 3, Loss : 3.546851634979248\n",
      "batch 4, Loss : 3.2864620685577393\n",
      "batch 5, Loss : 3.3008835315704346\n",
      "batch 6, Loss : 3.2625365257263184\n",
      "batch 7, Loss : 3.3129429817199707\n",
      "batch 8, Loss : 3.2393271923065186\n",
      "batch 9, Loss : 3.1395046710968018\n",
      "batch 10, Loss : 3.1759374141693115\n",
      "batch 11, Loss : 3.0591042041778564\n",
      "batch 12, Loss : 3.1147913932800293\n",
      "batch 13, Loss : 3.0061724185943604\n",
      "batch 14, Loss : 2.957833766937256\n",
      "batch 15, Loss : 2.9745867252349854\n",
      "batch 16, Loss : 2.9155819416046143\n",
      "batch 17, Loss : 2.9046926498413086\n",
      "batch 18, Loss : 2.9079174995422363\n",
      "batch 19, Loss : 2.8551058769226074\n",
      "batch 20, Loss : 2.902981758117676\n",
      "batch 21, Loss : 2.7915217876434326\n",
      "batch 22, Loss : 2.9033091068267822\n",
      "batch 23, Loss : 2.872188091278076\n",
      "batch 24, Loss : 2.851562023162842\n",
      "batch 25, Loss : 2.705273151397705\n",
      "batch 26, Loss : 2.8419995307922363\n",
      "batch 27, Loss : 2.75630521774292\n",
      "batch 28, Loss : 2.846946954727173\n",
      "batch 29, Loss : 2.84306001663208\n",
      "batch 30, Loss : 2.7829830646514893\n",
      "batch 31, Loss : 2.7021894454956055\n",
      "batch 32, Loss : 2.7663686275482178\n",
      "batch 33, Loss : 2.753434181213379\n",
      "batch 34, Loss : 2.7404847145080566\n",
      "batch 35, Loss : 2.669246196746826\n",
      "batch 36, Loss : 2.728076457977295\n",
      "batch 37, Loss : 2.779644012451172\n",
      "batch 38, Loss : 2.7451562881469727\n",
      "batch 39, Loss : 2.6774134635925293\n",
      "batch 40, Loss : 2.6474571228027344\n",
      "batch 41, Loss : 2.680433750152588\n",
      "batch 42, Loss : 2.7059593200683594\n",
      "batch 43, Loss : 2.7297873497009277\n",
      "batch 44, Loss : 2.6989188194274902\n",
      "batch 45, Loss : 2.6886794567108154\n",
      "batch 46, Loss : 2.616105318069458\n",
      "batch 47, Loss : 2.5111799240112305\n",
      "batch 48, Loss : 2.642554521560669\n",
      "batch 49, Loss : 2.7085013389587402\n",
      "batch 50, Loss : 2.661107063293457\n",
      "batch 51, Loss : 2.749019145965576\n",
      "batch 52, Loss : 2.7325384616851807\n",
      "batch 53, Loss : 2.642657518386841\n",
      "batch 54, Loss : 2.615992784500122\n",
      "batch 55, Loss : 2.6567578315734863\n",
      "batch 56, Loss : 2.603116989135742\n",
      "batch 57, Loss : 2.560851573944092\n",
      "batch 58, Loss : 2.671496868133545\n",
      "batch 59, Loss : 2.6365303993225098\n",
      "batch 60, Loss : 2.6592352390289307\n",
      "batch 61, Loss : 2.545872926712036\n",
      "batch 62, Loss : 2.6324403285980225\n",
      "batch 63, Loss : 2.6787238121032715\n",
      "batch 64, Loss : 2.60115647315979\n",
      "batch 65, Loss : 2.710841655731201\n",
      "batch 66, Loss : 2.5897278785705566\n",
      "batch 67, Loss : 2.530214786529541\n",
      "batch 68, Loss : 2.6461899280548096\n",
      "batch 69, Loss : 2.6300065517425537\n",
      "batch 70, Loss : 2.6352639198303223\n",
      "batch 71, Loss : 2.572434663772583\n",
      "batch 72, Loss : 2.5904366970062256\n",
      "batch 73, Loss : 2.5587449073791504\n",
      "batch 74, Loss : 2.583777904510498\n",
      "batch 75, Loss : 2.5733132362365723\n",
      "batch 76, Loss : 2.6194849014282227\n",
      "batch 77, Loss : 2.592323064804077\n",
      "batch 78, Loss : 2.5801842212677\n",
      "batch 79, Loss : 2.5859973430633545\n",
      "batch 80, Loss : 2.524366617202759\n",
      "batch 81, Loss : 2.6281256675720215\n",
      "batch 82, Loss : 2.6250929832458496\n",
      "batch 83, Loss : 2.5944437980651855\n",
      "batch 84, Loss : 2.570587158203125\n",
      "batch 85, Loss : 2.5651357173919678\n",
      "batch 86, Loss : 2.620393753051758\n",
      "batch 87, Loss : 2.5561861991882324\n",
      "batch 88, Loss : 2.5757718086242676\n",
      "batch 89, Loss : 2.5325939655303955\n",
      "batch 90, Loss : 2.540281295776367\n",
      "batch 91, Loss : 2.515312671661377\n",
      "batch 92, Loss : 2.5768439769744873\n",
      "batch 93, Loss : 2.5560827255249023\n",
      "batch 94, Loss : 2.5610759258270264\n",
      "batch 95, Loss : 2.5990028381347656\n",
      "batch 96, Loss : 2.639723777770996\n",
      "batch 97, Loss : 2.51068115234375\n",
      "batch 98, Loss : 2.5313594341278076\n",
      "batch 99, Loss : 2.5462441444396973\n",
      "batch 100, Loss : 2.6306419372558594\n",
      "batch 101, Loss : 2.5647664070129395\n",
      "batch 102, Loss : 2.5226521492004395\n",
      "batch 103, Loss : 2.6292569637298584\n",
      "batch 104, Loss : 2.506809711456299\n",
      "batch 105, Loss : 2.570749521255493\n",
      "batch 106, Loss : 2.564363479614258\n",
      "batch 107, Loss : 2.579951524734497\n",
      "batch 108, Loss : 2.4576828479766846\n",
      "batch 109, Loss : 2.5926740169525146\n",
      "batch 110, Loss : 2.496289014816284\n",
      "batch 111, Loss : 2.551506757736206\n",
      "batch 112, Loss : 2.514399290084839\n",
      "batch 113, Loss : 2.499560594558716\n",
      "batch 114, Loss : 2.5192580223083496\n",
      "batch 115, Loss : 2.5544276237487793\n",
      "batch 116, Loss : 2.5635976791381836\n",
      "batch 117, Loss : 2.5092978477478027\n",
      "batch 118, Loss : 2.578603506088257\n",
      "batch 119, Loss : 2.5564234256744385\n",
      "batch 120, Loss : 2.6301493644714355\n",
      "batch 121, Loss : 2.4553117752075195\n",
      "batch 122, Loss : 2.544943332672119\n",
      "batch 123, Loss : 2.483112335205078\n",
      "batch 124, Loss : 2.5380136966705322\n",
      "batch 125, Loss : 2.5407004356384277\n",
      "batch 126, Loss : 2.578517436981201\n",
      "batch 127, Loss : 2.5448379516601562\n",
      "batch 128, Loss : 2.4761719703674316\n",
      "batch 129, Loss : 2.445265769958496\n",
      "batch 130, Loss : 2.5252981185913086\n",
      "batch 131, Loss : 2.5533499717712402\n",
      "batch 132, Loss : 2.5584139823913574\n",
      "batch 133, Loss : 2.5441479682922363\n",
      "batch 134, Loss : 2.5415568351745605\n",
      "batch 135, Loss : 2.5693087577819824\n",
      "batch 136, Loss : 2.532874822616577\n",
      "batch 137, Loss : 2.5184357166290283\n",
      "batch 138, Loss : 2.5461108684539795\n",
      "batch 139, Loss : 2.5610833168029785\n",
      "batch 140, Loss : 2.567202091217041\n",
      "batch 141, Loss : 2.5707709789276123\n",
      "batch 142, Loss : 2.572838306427002\n",
      "batch 143, Loss : 2.524482011795044\n",
      "batch 144, Loss : 2.5963246822357178\n",
      "batch 145, Loss : 2.5561108589172363\n",
      "batch 146, Loss : 2.541761875152588\n",
      "batch 147, Loss : 2.524069309234619\n",
      "batch 148, Loss : 2.5895276069641113\n",
      "batch 149, Loss : 2.506457567214966\n",
      "batch 150, Loss : 2.5418410301208496\n",
      "batch 151, Loss : 2.524811267852783\n",
      "batch 152, Loss : 2.553926944732666\n",
      "batch 153, Loss : 2.514601230621338\n",
      "batch 154, Loss : 2.5666584968566895\n",
      "batch 155, Loss : 2.5646917819976807\n",
      "batch 156, Loss : 2.5120370388031006\n",
      "batch 157, Loss : 2.5455808639526367\n",
      "batch 158, Loss : 2.5503859519958496\n",
      "batch 159, Loss : 2.532285451889038\n",
      "batch 160, Loss : 2.566195011138916\n",
      "batch 161, Loss : 2.6323187351226807\n",
      "batch 162, Loss : 2.553213596343994\n",
      "batch 163, Loss : 2.5500988960266113\n",
      "batch 164, Loss : 2.5062716007232666\n",
      "batch 165, Loss : 2.5230205059051514\n",
      "batch 166, Loss : 2.5157456398010254\n",
      "batch 167, Loss : 2.5616157054901123\n",
      "batch 168, Loss : 2.4591290950775146\n",
      "batch 169, Loss : 2.463785171508789\n",
      "batch 170, Loss : 2.4965620040893555\n",
      "batch 171, Loss : 2.5274901390075684\n",
      "batch 172, Loss : 2.5316452980041504\n",
      "batch 173, Loss : 2.5027287006378174\n",
      "batch 174, Loss : 2.488070249557495\n",
      "batch 175, Loss : 2.512939691543579\n",
      "batch 176, Loss : 2.5075936317443848\n",
      "batch 177, Loss : 2.4614815711975098\n",
      "batch 178, Loss : 2.4829251766204834\n",
      "batch 179, Loss : 2.4935600757598877\n",
      "batch 180, Loss : 2.5286126136779785\n",
      "batch 181, Loss : 2.546567916870117\n",
      "batch 182, Loss : 2.5351951122283936\n",
      "batch 183, Loss : 2.553318500518799\n",
      "batch 184, Loss : 2.5615029335021973\n",
      "batch 185, Loss : 2.487797260284424\n",
      "batch 186, Loss : 2.4967072010040283\n",
      "batch 187, Loss : 2.5383968353271484\n",
      "batch 188, Loss : 2.503262519836426\n",
      "batch 189, Loss : 2.5376696586608887\n",
      "batch 190, Loss : 2.560605049133301\n",
      "batch 191, Loss : 2.4636731147766113\n",
      "batch 192, Loss : 2.495856761932373\n",
      "batch 193, Loss : 2.494966745376587\n",
      "batch 194, Loss : 2.5057997703552246\n",
      "batch 195, Loss : 2.5043632984161377\n",
      "batch 196, Loss : 2.54130220413208\n",
      "batch 197, Loss : 2.484910726547241\n",
      "batch 198, Loss : 2.5181212425231934\n",
      "batch 199, Loss : 2.567079782485962\n",
      "batch 200, Loss : 2.59505033493042\n",
      "batch 201, Loss : 2.474188804626465\n",
      "batch 202, Loss : 2.606304407119751\n",
      "batch 203, Loss : 2.4810218811035156\n",
      "batch 204, Loss : 2.4817357063293457\n",
      "batch 205, Loss : 2.5711262226104736\n",
      "batch 206, Loss : 2.5084707736968994\n",
      "batch 207, Loss : 2.5756630897521973\n",
      "batch 208, Loss : 2.5334279537200928\n",
      "batch 209, Loss : 2.515928030014038\n",
      "batch 210, Loss : 2.4745638370513916\n",
      "batch 211, Loss : 2.6084530353546143\n",
      "batch 212, Loss : 2.4824366569519043\n",
      "batch 213, Loss : 2.569185495376587\n",
      "batch 214, Loss : 2.536651849746704\n",
      "batch 215, Loss : 2.536087989807129\n",
      "batch 216, Loss : 2.5219531059265137\n",
      "batch 217, Loss : 2.490111827850342\n",
      "batch 218, Loss : 2.4937586784362793\n",
      "batch 219, Loss : 2.543764114379883\n",
      "batch 220, Loss : 2.529872417449951\n",
      "batch 221, Loss : 2.523027181625366\n",
      "batch 222, Loss : 2.550747871398926\n",
      "batch 223, Loss : 2.551229953765869\n",
      "batch 224, Loss : 2.521730422973633\n",
      "batch 225, Loss : 2.502589225769043\n",
      "batch 226, Loss : 2.589738130569458\n",
      "batch 227, Loss : 2.532982349395752\n",
      "batch 228, Loss : 2.59908390045166\n",
      "batch 229, Loss : 2.5100762844085693\n",
      "batch 230, Loss : 2.525141477584839\n",
      "batch 231, Loss : 2.526449203491211\n",
      "batch 232, Loss : 2.438786745071411\n",
      "batch 233, Loss : 2.469649314880371\n",
      "batch 234, Loss : 2.4859957695007324\n",
      "batch 235, Loss : 2.5468573570251465\n",
      "batch 236, Loss : 2.4925851821899414\n",
      "batch 237, Loss : 2.544811964035034\n",
      "batch 238, Loss : 2.481968402862549\n",
      "batch 239, Loss : 2.5916967391967773\n",
      "batch 240, Loss : 2.42061448097229\n",
      "batch 241, Loss : 2.51790189743042\n",
      "batch 242, Loss : 2.476468563079834\n",
      "batch 243, Loss : 2.461885690689087\n",
      "batch 244, Loss : 2.5242791175842285\n",
      "batch 245, Loss : 2.6076579093933105\n",
      "batch 246, Loss : 2.5629055500030518\n",
      "batch 247, Loss : 2.525721549987793\n",
      "batch 248, Loss : 2.4669928550720215\n",
      "batch 249, Loss : 2.5045461654663086\n",
      "batch 250, Loss : 2.4860928058624268\n",
      "batch 251, Loss : 2.5194826126098633\n",
      "batch 252, Loss : 2.441920042037964\n",
      "batch 253, Loss : 2.563534736633301\n",
      "batch 254, Loss : 2.479027509689331\n",
      "batch 255, Loss : 2.4895741939544678\n",
      "batch 256, Loss : 2.533229351043701\n",
      "batch 257, Loss : 2.4828951358795166\n",
      "batch 258, Loss : 2.5333189964294434\n",
      "batch 259, Loss : 2.4164648056030273\n",
      "batch 260, Loss : 2.5123705863952637\n",
      "batch 261, Loss : 2.4953086376190186\n",
      "batch 262, Loss : 2.4704203605651855\n",
      "batch 263, Loss : 2.456902503967285\n",
      "batch 264, Loss : 2.496464252471924\n",
      "batch 265, Loss : 2.5834274291992188\n",
      "batch 266, Loss : 2.4615416526794434\n",
      "batch 267, Loss : 2.496737241744995\n",
      "batch 268, Loss : 2.4573256969451904\n",
      "batch 269, Loss : 2.5604248046875\n",
      "batch 270, Loss : 2.5092697143554688\n",
      "batch 271, Loss : 2.4440677165985107\n",
      "batch 272, Loss : 2.526552438735962\n",
      "batch 273, Loss : 2.4678423404693604\n",
      "batch 274, Loss : 2.4940311908721924\n",
      "batch 275, Loss : 2.515784740447998\n",
      "batch 276, Loss : 2.482579231262207\n",
      "batch 277, Loss : 2.4942538738250732\n",
      "batch 278, Loss : 2.515388250350952\n",
      "batch 279, Loss : 2.5173308849334717\n",
      "batch 280, Loss : 2.475165605545044\n",
      "batch 281, Loss : 2.475902795791626\n",
      "batch 282, Loss : 2.490166187286377\n",
      "batch 283, Loss : 2.456118583679199\n",
      "batch 284, Loss : 2.4731690883636475\n",
      "batch 285, Loss : 2.6088175773620605\n",
      "batch 286, Loss : 2.4296951293945312\n",
      "batch 287, Loss : 2.4869589805603027\n",
      "batch 288, Loss : 2.4849462509155273\n",
      "batch 289, Loss : 2.4540038108825684\n",
      "batch 290, Loss : 2.5428271293640137\n",
      "batch 291, Loss : 2.5222907066345215\n",
      "batch 292, Loss : 2.439995288848877\n",
      "batch 293, Loss : 2.477728843688965\n",
      "batch 294, Loss : 2.517960786819458\n",
      "batch 295, Loss : 2.5240211486816406\n",
      "batch 296, Loss : 2.475349187850952\n",
      "batch 297, Loss : 2.49769926071167\n",
      "batch 298, Loss : 2.4747209548950195\n",
      "batch 299, Loss : 2.493687152862549\n",
      "batch 300, Loss : 2.463010549545288\n",
      "batch 301, Loss : 2.530442714691162\n",
      "batch 302, Loss : 2.479583978652954\n",
      "batch 303, Loss : 2.5143024921417236\n",
      "batch 304, Loss : 2.453538417816162\n",
      "batch 305, Loss : 2.5294392108917236\n",
      "batch 306, Loss : 2.480854034423828\n",
      "batch 307, Loss : 2.477484703063965\n",
      "batch 308, Loss : 2.5074474811553955\n",
      "batch 309, Loss : 2.5378904342651367\n",
      "batch 310, Loss : 2.481825113296509\n",
      "batch 311, Loss : 2.4776201248168945\n",
      "batch 312, Loss : 2.490921974182129\n",
      "batch 313, Loss : 2.540273666381836\n",
      "batch 314, Loss : 2.5433437824249268\n",
      "batch 315, Loss : 2.4578487873077393\n",
      "batch 316, Loss : 2.446279764175415\n",
      "batch 317, Loss : 2.466505527496338\n",
      "batch 318, Loss : 2.496150493621826\n",
      "batch 319, Loss : 2.433241367340088\n",
      "batch 320, Loss : 2.4990592002868652\n",
      "batch 321, Loss : 2.4280478954315186\n",
      "batch 322, Loss : 2.497957706451416\n",
      "batch 323, Loss : 2.4806621074676514\n",
      "batch 324, Loss : 2.5477957725524902\n",
      "batch 325, Loss : 2.533165693283081\n",
      "batch 326, Loss : 2.469482183456421\n",
      "batch 327, Loss : 2.5335726737976074\n",
      "batch 328, Loss : 2.502746105194092\n",
      "batch 329, Loss : 2.475668430328369\n",
      "batch 330, Loss : 2.4649338722229004\n",
      "batch 331, Loss : 2.539808511734009\n",
      "batch 332, Loss : 2.5657360553741455\n",
      "batch 333, Loss : 2.4747567176818848\n",
      "batch 334, Loss : 2.3919625282287598\n",
      "batch 335, Loss : 2.535248041152954\n",
      "batch 336, Loss : 2.514486074447632\n",
      "batch 337, Loss : 2.5361149311065674\n",
      "batch 338, Loss : 2.502413272857666\n",
      "batch 339, Loss : 2.4565072059631348\n",
      "batch 340, Loss : 2.5312697887420654\n",
      "batch 341, Loss : 2.4790115356445312\n",
      "batch 342, Loss : 2.43391752243042\n",
      "batch 343, Loss : 2.4622764587402344\n",
      "batch 344, Loss : 2.49113130569458\n",
      "batch 345, Loss : 2.536729335784912\n",
      "batch 346, Loss : 2.493631601333618\n",
      "batch 347, Loss : 2.562194347381592\n",
      "batch 348, Loss : 2.452979564666748\n",
      "batch 349, Loss : 2.513667106628418\n",
      "batch 350, Loss : 2.4544999599456787\n",
      "batch 351, Loss : 2.5233378410339355\n",
      "batch 352, Loss : 2.4747610092163086\n",
      "batch 353, Loss : 2.486618995666504\n",
      "batch 354, Loss : 2.462923526763916\n",
      "batch 355, Loss : 2.5320308208465576\n",
      "batch 356, Loss : 2.47975754737854\n",
      "batch 357, Loss : 2.480257749557495\n",
      "batch 358, Loss : 2.4711763858795166\n",
      "batch 359, Loss : 2.4966158866882324\n",
      "batch 360, Loss : 2.557417154312134\n",
      "batch 361, Loss : 2.508124828338623\n",
      "batch 362, Loss : 2.50815486907959\n",
      "batch 363, Loss : 2.5422122478485107\n",
      "batch 364, Loss : 2.4905848503112793\n",
      "batch 365, Loss : 2.5302369594573975\n",
      "batch 366, Loss : 2.513359308242798\n",
      "batch 367, Loss : 2.460120916366577\n",
      "batch 368, Loss : 2.5271735191345215\n",
      "batch 369, Loss : 2.469348430633545\n",
      "batch 370, Loss : 2.5160484313964844\n",
      "batch 371, Loss : 2.4420580863952637\n",
      "batch 372, Loss : 2.5616941452026367\n",
      "batch 373, Loss : 2.498856782913208\n",
      "batch 374, Loss : 2.5559515953063965\n",
      "batch 375, Loss : 2.498896837234497\n",
      "batch 376, Loss : 2.5442585945129395\n",
      "batch 377, Loss : 2.5228357315063477\n",
      "batch 378, Loss : 2.482876777648926\n",
      "batch 379, Loss : 2.4478535652160645\n",
      "batch 380, Loss : 2.5702900886535645\n",
      "batch 381, Loss : 2.5088798999786377\n",
      "batch 382, Loss : 2.5177810192108154\n",
      "batch 383, Loss : 2.509765863418579\n",
      "batch 384, Loss : 2.5363810062408447\n",
      "batch 385, Loss : 2.505448818206787\n",
      "batch 386, Loss : 2.5343029499053955\n",
      "batch 387, Loss : 2.4749865531921387\n",
      "batch 388, Loss : 2.473268985748291\n",
      "batch 389, Loss : 2.4692327976226807\n",
      "batch 390, Loss : 2.513277530670166\n",
      "batch 391, Loss : 2.560106039047241\n",
      "batch 392, Loss : 2.4388420581817627\n",
      "batch 393, Loss : 2.5324435234069824\n",
      "batch 394, Loss : 2.5084176063537598\n",
      "batch 395, Loss : 2.504035472869873\n",
      "batch 396, Loss : 2.4993646144866943\n",
      "batch 397, Loss : 2.5076370239257812\n",
      "batch 398, Loss : 2.5230941772460938\n",
      "batch 399, Loss : 2.646387815475464\n",
      "batch 400, Loss : 2.5121421813964844\n",
      "batch 401, Loss : 2.4732441902160645\n",
      "batch 402, Loss : 2.5045881271362305\n",
      "batch 403, Loss : 2.5847184658050537\n",
      "batch 404, Loss : 2.4339983463287354\n",
      "batch 405, Loss : 2.5125784873962402\n",
      "batch 406, Loss : 2.495321750640869\n",
      "batch 407, Loss : 2.5288891792297363\n",
      "batch 408, Loss : 2.467073917388916\n",
      "batch 409, Loss : 2.4856982231140137\n",
      "batch 410, Loss : 2.4627292156219482\n",
      "batch 411, Loss : 2.4524569511413574\n",
      "batch 412, Loss : 2.4646408557891846\n",
      "batch 413, Loss : 2.5113682746887207\n",
      "batch 414, Loss : 2.475848913192749\n",
      "batch 415, Loss : 2.5146164894104004\n",
      "batch 416, Loss : 2.4625449180603027\n",
      "batch 417, Loss : 2.4620614051818848\n",
      "batch 418, Loss : 2.44907283782959\n",
      "batch 419, Loss : 2.4427714347839355\n",
      "batch 420, Loss : 2.4117252826690674\n",
      "batch 421, Loss : 2.5222926139831543\n",
      "batch 422, Loss : 2.4672162532806396\n",
      "batch 423, Loss : 2.4751906394958496\n",
      "batch 424, Loss : 2.526444911956787\n",
      "batch 425, Loss : 2.5000410079956055\n",
      "batch 426, Loss : 2.511446237564087\n",
      "batch 427, Loss : 2.4277310371398926\n",
      "batch 428, Loss : 2.516416072845459\n",
      "batch 429, Loss : 2.4604525566101074\n",
      "batch 430, Loss : 2.4589571952819824\n",
      "batch 431, Loss : 2.4670865535736084\n",
      "batch 432, Loss : 2.4320285320281982\n",
      "batch 433, Loss : 2.478271007537842\n",
      "batch 434, Loss : 2.4936492443084717\n",
      "batch 435, Loss : 2.5202584266662598\n",
      "batch 436, Loss : 2.517395257949829\n",
      "batch 437, Loss : 2.5916121006011963\n",
      "batch 438, Loss : 2.447068214416504\n",
      "batch 439, Loss : 2.4690587520599365\n",
      "batch 440, Loss : 2.4186127185821533\n",
      "batch 441, Loss : 2.5386312007904053\n",
      "batch 442, Loss : 2.416785478591919\n",
      "batch 443, Loss : 2.5452849864959717\n",
      "batch 444, Loss : 2.4695146083831787\n",
      "batch 445, Loss : 2.4456124305725098\n",
      "batch 446, Loss : 2.495227813720703\n",
      "batch 447, Loss : 2.491079092025757\n",
      "batch 448, Loss : 2.50058650970459\n",
      "batch 449, Loss : 2.40956449508667\n",
      "batch 450, Loss : 2.4799370765686035\n",
      "batch 451, Loss : 2.487074136734009\n",
      "batch 452, Loss : 2.513927698135376\n",
      "batch 453, Loss : 2.537332534790039\n",
      "batch 454, Loss : 2.5086495876312256\n",
      "batch 455, Loss : 2.4622769355773926\n",
      "batch 456, Loss : 2.5282809734344482\n",
      "batch 457, Loss : 2.517807722091675\n",
      "batch 458, Loss : 2.483306407928467\n",
      "batch 459, Loss : 2.548893928527832\n",
      "batch 460, Loss : 2.4369828701019287\n",
      "batch 461, Loss : 2.418614625930786\n",
      "batch 462, Loss : 2.4720466136932373\n",
      "batch 463, Loss : 2.403862476348877\n",
      "batch 464, Loss : 2.5620369911193848\n",
      "batch 465, Loss : 2.5176632404327393\n",
      "batch 466, Loss : 2.444373846054077\n",
      "batch 467, Loss : 2.5226426124572754\n",
      "batch 468, Loss : 2.4909064769744873\n",
      "batch 469, Loss : 2.511596441268921\n",
      "batch 470, Loss : 2.4529480934143066\n",
      "batch 471, Loss : 2.473494052886963\n",
      "batch 472, Loss : 2.4980568885803223\n",
      "batch 473, Loss : 2.4159412384033203\n",
      "batch 474, Loss : 2.5287363529205322\n",
      "batch 475, Loss : 2.49049711227417\n",
      "batch 476, Loss : 2.4535045623779297\n",
      "batch 477, Loss : 2.4044723510742188\n",
      "batch 478, Loss : 2.4417431354522705\n",
      "batch 479, Loss : 2.4454872608184814\n",
      "batch 480, Loss : 2.4933953285217285\n",
      "batch 481, Loss : 2.457397222518921\n",
      "batch 482, Loss : 2.4583516120910645\n",
      "batch 483, Loss : 2.5299010276794434\n",
      "batch 484, Loss : 2.505528450012207\n",
      "batch 485, Loss : 2.4770708084106445\n",
      "batch 486, Loss : 2.527334213256836\n",
      "batch 487, Loss : 2.4676339626312256\n",
      "batch 488, Loss : 2.495595693588257\n",
      "batch 489, Loss : 2.5077531337738037\n",
      "batch 490, Loss : 2.4562292098999023\n",
      "batch 491, Loss : 2.49334979057312\n",
      "batch 492, Loss : 2.4858431816101074\n",
      "batch 493, Loss : 2.4772980213165283\n",
      "batch 494, Loss : 2.519639730453491\n",
      "batch 495, Loss : 2.4482407569885254\n",
      "batch 496, Loss : 2.4379568099975586\n",
      "batch 497, Loss : 2.498595952987671\n",
      "batch 498, Loss : 2.468140125274658\n",
      "batch 499, Loss : 2.463205575942993\n",
      "batch 500, Loss : 2.5765841007232666\n",
      "batch 501, Loss : 2.406001091003418\n",
      "batch 502, Loss : 2.4823923110961914\n",
      "batch 503, Loss : 2.4706673622131348\n",
      "batch 504, Loss : 2.49013090133667\n",
      "batch 505, Loss : 2.490241527557373\n",
      "batch 506, Loss : 2.5377047061920166\n",
      "batch 507, Loss : 2.557694435119629\n",
      "batch 508, Loss : 2.4412431716918945\n",
      "batch 509, Loss : 2.462475538253784\n",
      "batch 510, Loss : 2.485191822052002\n",
      "batch 511, Loss : 2.4805917739868164\n",
      "batch 512, Loss : 2.4743270874023438\n",
      "batch 513, Loss : 2.500776767730713\n",
      "batch 514, Loss : 2.505690336227417\n",
      "batch 515, Loss : 2.428334951400757\n",
      "batch 516, Loss : 2.441847085952759\n",
      "batch 517, Loss : 2.550431251525879\n",
      "batch 518, Loss : 2.4924368858337402\n",
      "batch 519, Loss : 2.459261417388916\n",
      "batch 520, Loss : 2.47798490524292\n",
      "batch 521, Loss : 2.495431900024414\n",
      "batch 522, Loss : 2.5201056003570557\n",
      "batch 523, Loss : 2.550020694732666\n",
      "batch 524, Loss : 2.4755654335021973\n",
      "batch 525, Loss : 2.465977191925049\n",
      "batch 526, Loss : 2.4715776443481445\n",
      "batch 527, Loss : 2.5334713459014893\n",
      "batch 528, Loss : 2.5389137268066406\n",
      "batch 529, Loss : 2.46806001663208\n",
      "batch 530, Loss : 2.47224497795105\n",
      "batch 531, Loss : 2.469188928604126\n",
      "batch 532, Loss : 2.3704917430877686\n",
      "batch 533, Loss : 2.4895741939544678\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model, losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/edu/gordon_ms/Sem3/DeepLearning/Shakespeak/model.py:66\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, loader, tokenizer)\u001b[0m\n\u001b[1;32m     61\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(\n\u001b[1;32m     62\u001b[0m     logits\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m),\n\u001b[1;32m     63\u001b[0m     targets\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mlong()\u001b[38;5;241m.\u001b[39mto(model\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     64\u001b[0m )\n\u001b[1;32m     65\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m---> 66\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m     70\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.11/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.11/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model, losses = train_model(model, loader, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT(\n",
      "  (WTE): Embedding(65, 384)\n",
      "  (WPE): WPE()\n",
      "  (blocks): ModuleList(\n",
      "    (0-5): 6 x Block(\n",
      "      (CausalSelfAttn): QKVAttention()\n",
      "      (W1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "      (W2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "      (LayerNorm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (LayerNorm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (Dropout): Dropout(p=0.2, inplace=False)\n",
      "  (Final_LayerNorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "  (LM_Head): Linear(in_features=384, out_features=65, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "cpkt = torch.load(\"./runs/model_16700.pt\", map_location=torch.device('cpu'))\n",
    "model.load_state_dict(cpkt)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gordon/Documents/edu/gordon_ms/Sem3/DeepLearning/Shakespeak/utils.py:94: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  new_c = torch.tensor(char_id).reshape(shape=(1, 1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let us kill him, and we'll have corn at our own price.! What then?\n",
      "\n",
      "EXTON:\n",
      "A sister, manage that he mood this in father, no,\n",
      "teating hardon'd spitut!\n",
      "\n",
      "POMPEY:\n",
      "The sends he selfsion that doublet this time maid,\n",
      "becauses an hour of Richard have speak should shop caught\n",
      "intercliner'd to fixe yound, I there executor in unnto yand\n",
      "to his cate, an aid evil, yexathing'd's walvicke, hid;\n",
      "Whatell may nother the umourddger dour\n",
      "An, lifthere mere, nour nagn,\n",
      "Wher ine's, inghee? dd; munoun'd Than, whindrst,\n",
      "St\n",
      "Stoun meagooune? har.'st, y thinou! wakid iffffft\n",
      "P\n"
     ]
    }
   ],
   "source": [
    "from utils import generate\n",
    "\n",
    "new_tokens = generate(model, dataset.encode(\"Let us kill him, and we'll have corn at our own price.!\"), 500)\n",
    "print(dataset.decode(new_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
